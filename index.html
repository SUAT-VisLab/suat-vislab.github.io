---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: main_page
title: VisLab@SUAT
permalink: /
---

  <!-- Header -->
  <header class="masthead">
    <div class="container d-flex h-100 align-items-center">
      <div class="mx-auto text-center">
        <h3 class="text-white-50 mx-auto mt-2 mb-5">Welcome to Vision-Lab@SUAT</h3>
      </div>
    </div>
  </header>

  <section  class="projects-section bg-light">
    <div class="container">

      <!-- About Row -->
    <div id="About" class="row align-items-center no-gutters mb-4 mb-lg-5 section-container">
      <div class="col-xl-12 col-lg-12">
        <div class="featured-text text-lg-left">
          <h4>About</h4>
    
          <p>
            We are the Vision Laboratory at Shenzhen University of Advanced Technology (VisLab-SUAT).
            Our research interests span a wide range of topics in visual intelligence, including:
          </p>
    
          <ul style="line-height: 1.8;">
            <li><strong>Computer Vision</strong>: object detection, tracking, segmentation, pose estimation, diffusion model</li>
            <li><strong>Video Understanding</strong>: action recognition, temporal grounding, multi-human association</li>
            <li><strong>Multimodal Foundation Models</strong>: vision-language alignment, prompt-based segmentation, large vision models</li>
            <li><strong>Image Processing</strong>: low-light enhancement, super-resolution, image translation</li>
            <li><strong>Machine Vision & Robotics</strong>: robotic perception, autonomous systems</li>
          </ul>
    
          <p>
            We are a dynamic and interdisciplinary team committed to advancing the frontiers of visual intelligence.
            By combining theoretical innovation with practical application, we aim to build intelligent systems that can perceive, reason, and act in complex real-world environments.
          </p>
    
          <p>
            At VisLab, we emphasize academic excellence, open collaboration, and real-world impact.
            Our work spans from algorithm design to system deployment, with applications in robotics, smart cities, autonomous vehicles, and beyond.
          </p>
        </div>
      </div>
    </div>


      <!-- Faculty -->
      <div id="Faculty" class="row align-items-center no-gutters mb-4 mb-lg-5 section-container">

        <div class="col-xl-12 col-lg-12">
          <div class="featured-text text-lg-left">
            <h4>Faculty</h4>
            
            {% include faculty_cards.html faculty=site.data.faculty %}

          </div>
        </div>
      </div>
      
      
      <!-- Students -->
      <div id="Students" class="row align-items-center no-gutters mb-4 mb-lg-5 section-container">

        <div class="col-xl-12 col-lg-12">
          <div class="featured-text text-lg-left">
            <h4>Students</h4>

            <h5>PhD students</h5>

            {% include student_cards.html students=site.data.students.phd %}

            <h5>MS students</h5>

            {% include student_cards.html students=site.data.students.ms %}
            
            <h5>BS students</h5>

            {% include student_cards.html students=site.data.students.bs %}

            <h5>Research Assistants</h5>

            {% include student_cards.html students=site.data.students.ra %}

          </div>
        </div>
      </div>

      <!-- Alumni -->
      <div id="Alumni" class="row align-items-center no-gutters mb-4 mb-lg-5 section-container">

        <div class="col-xl-12 col-lg-12">
          <div class="featured-text text-lg-left">
            <h4>Alumni</h4>
             Coming Soon...
<!--             <h5>PhD Alumni</h5>

            {% include alumni_cards.html alumni=site.data.alumni.phd prefix="Dr." %}

            <h5>MS Alumni</h5>

            {% include alumni_cards.html alumni=site.data.alumni.ms %}
 -->
          </div>
        </div>
      </div>

   <!-- Research Publications with Scroll -->
<div id="Research" class="row align-items-center no-gutters mb-4 mb-lg-5 section-container">
  <div class="col-xl-12 col-lg-12">
    <div class="featured-text text-lg-left">
      <h4>Selected Publications</h4>

      <!-- Scrollable List -->
      <div style="max-height: 500px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 8px;">
        <ul class="list-group list-group-flush" style="font-size: 0.95rem;">

          <!-- 2025 -->
          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>A New Benchmark and Algorithm for Clothes-Changing Video Person Re-Identification</strong><br>
              <span class="text-muted">L. Wang, X. Zhang, <strong>R. Han</strong>, Y. Wei, <strong>S. Wang</strong>, W. Feng · IEEE TIFS, 2025</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>From Indoor to Outdoor: Unsupervised Domain Adaptive Gait Recognition</strong><br>
              <span class="text-muted">L. Wang, W. Feng, <strong>R. Han</strong>, X. Zhang, Y. Wei, <strong>S. Wang</strong> · Pattern Recognition, 2025</span>
            </div>
            <span class="badge badge-success badge-pill mt-1">CCF B</span>
          </li>

          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Synthetic-To-Real Video Person Re-ID</strong><br>
              <span class="text-muted">X. Zhang, <strong>R. Han</strong>, L. Wang, L. Song, J. Hou, W. Feng · IEEE TIFS, 2025</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>A Large-Scale Combinatorial Benchmark for Sign Language Recognition</strong><br>
              <span class="text-muted">L. Gao, L. Wan, L. Hu, <strong>R. Han</strong>, Z. Liu, P. Shi, F. Shang, W. Feng · Pattern Recognition, 2025</span>
            </div>
            <span class="badge badge-success badge-pill mt-1">CCF B</span>
          </li>

          <!-- 2024 -->
          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Rethinking the One-shot Object Detection: Cross-Domain Object Search</strong><br>
              <span class="text-muted">Y. Zhang, S. Zheng, <strong>R. Han</strong>, Y. Feng, J. Hou, L. Song, W. Feng, L. Wan · ACM MM, 2024</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Ovt-b: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking</strong><br>
              <span class="text-muted">H. Liang, <strong>R. Han</strong> · NeurIPS, 2024</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Benchmarking the Complementary-View Multi-human Association and Tracking</strong><br>
              <span class="text-muted"><strong>R. Han</strong>, W. Feng, F. Wang, et al. · IJCV, 2024</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <!-- 2023 -->
          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Relating View Directions of Complementary-View Mobile Cameras via the Human Shadow</strong><br>
              <span class="text-muted"><strong>R. Han</strong>, Y. Gan, L. Wang, N. Li, W. Feng, <strong>S. Wang</strong> · IJCV, 2023</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <!-- 2022 -->
          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Panoramic Human Activity Recognition</strong><br>
              <span class="text-muted"><strong>R. Han</strong>, H. Yan, J. Li, S. Wang, W. Feng, <strong>S. Wang</strong> · ECCV, 2022</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Connecting the Complementary-View Videos: Joint Camera Identification and Subject Association</strong><br>
              <span class="text-muted"><strong>R. Han</strong>, Y. Gan, J. Li, F. Wang, W. Feng, <strong>S. Wang</strong> · CVPR, 2022</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

          <!-- 2021 -->
          <li class="list-group-item d-flex justify-content-between align-items-start">
            <div>
              <strong>Multiple Human Association and Tracking from Egocentric and Complementary Top Views</strong><br>
              <span class="text-muted"><strong>R. Han</strong>, W. Feng, Y. Zhang, J. Zhao, <strong>S. Wang</strong> · TPAMI, 2021</span>
            </div>
            <span class="badge badge-primary badge-pill mt-1">CCF A</span>
          </li>

        </ul>
      </div>
    </div>
  </div>
</div>



    </div>
  </section>
